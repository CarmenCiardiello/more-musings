[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Carmen Ciardiello. I am an engineer who has worked in a MLB front office as a Research & Development Analyst and have written at FanGraphs as a part-time contributor. You can easily look back at my FanGraphs archive by searching for my name followed by FanGraphs. Some of my early research can be found at sabermetricmusings.blogspot.com, which can be accessed by clicking on the archive icon.\nFeel free to contact me on Twitter or email me at cciardiello1@gmail.com with any questions or concerns regarding my work on this site or anything else that may cross your mind. I am always happy to discuss baseball, football, hockey, programming, and all combinations thereof."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sporting Musings",
    "section": "",
    "text": "The Importance of Powerplay Offense\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nScore-Effect Remnants\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nCan We Identify Poor Postseason Performers?\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nCountering Opponent Flurries\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nCommentary on Secondary Assists\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nNHL Primary Matchup Utilization\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nRobert Stephenson’s Cutter\n\n\n\n\n\n\n\nbaseball\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nDeveloping a Predictive wOBA Measure\n\n\n\n\n\n\n\nbaseball\n\n\nanalysis\n\n\nmodeling\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nHockey Microstats Repeatability\n\n\n\n\n\n\n\nhockey\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\n  \n\n\n\n\nHitter Velocity Splits\n\n\n\n\n\n\n\nbaseball\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nCarmen Ciardiello\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/velocity-splits/index.html",
    "href": "posts/velocity-splits/index.html",
    "title": "Hitter Velocity Splits",
    "section": "",
    "text": "One avenue of analysis I see most around the postseason when individual batter-pitcher matchups or team matchups are more heavily scrutinized is bucketing offensive performance by velocity faced, specifically against fastballs. An analyst might posit “This bullpen has thrown X percent of its fastballs over 95 mph and this team has produced a Y wOBA on such pitches.” Or something along the lines of “John Doe is going to have a tough go of it against this teams cadre of high octane arms; he has posted an X wOBA on fastballs over 95 mph, versus Y on fastballs below that threshold.” Similar points might be made in an analysis of a player in the midst of a slump during the championship season (instances that come to mind are various pieces related to Jose Abreu from 2023, for example). In all of these cases, the performance against what many would call “premium velocity” or “95+”, which at this point in time I would argue does not constitute premium velocity (reminds me of this Effectively Wild episode), is certainly descriptive. We take what happened in the past, create a cutoff (albeit an arbitrary one), and calculate the respective wOBAs.\nThat is all well and good, a description of that happened in the past. But I have few points of contention with type of analysis, either at the player or team level. The first, which I have already alluded to, is the arbitrary point at which most decide to bucket the data. This is almost always 95 mph. I am not aware of why this became the accepted line of demarcation; I would guess it just looks like a nice round number. Or maybe it is related to MLB’s definition of a hard hit ball, also 95 mph. Nevertheless, I can appreciate the desire to bucket information for ease of analysis or understanding. Modelling this in a continuous manner and presenting the results is often a more cumbersome endeavor and may lose some of the readers. A trivial comparison of performance on either side of a cutoff is more digestible. I suggest in the future, maybe we use cutoff based on velocity percentile? League-wide velocity is not a static figure, thus the significance of the 95 mph figure is different in say 2023 versus 2015.\nThe arbitrary buckets are one issue, but like I said I can understand the impetus for wanting to structure analysis in this way. My main issue the the usefulness of this information altogether, in that I am very skeptical that this sort of analysis yields valuable insights into performance in the past or future. The usage of wOBA as the measure of choice belies a player/team’s performance against higher velocity pitches because it only considers pitches that mark the end of a plate appearance. All of the fastballs that a player swings through with either zero or one strike are not considered at all. Furthermore, using wOBA muddies the picture by not distinguishing whether a player/team struggles with making contact or producing damage on contact. If you want to demonstrate an effectiveness (or lack thereof) against velocity, these two skills need to be separated out. The easiest method would be to consider whiff or swinging strike rate and wOBACON (or some measure concerned solely with contact quality). This criticism does not apply to all analyses of this type, but it is something that I see quite often and think needs to be adjudicated. I understand this distinction can be a bit tricky. It requires a few different Savant searches and some spreadsheet jockeying. But failing to make this distinction prohibits any conclusions one draws from being revealing.\nNow, let us move on to the functionality of bucketing performance by velocity faced, even after making the distinction between contact and contact quality. Looking at contact (in the form of swinging strike rate) or contact quality (in the form of wOBACON) there has been been a standard bell curve for deviations in performance against higher and lower velocity, looking solely at players who accumulated 100 plate appearances in a given season since 2015.\n\n\n\n\n\nThere are a few conclusions we can draw from these figures. First, these look like standard Gaussian distributions. The swinging strike rate figure is centered close to around 0.05 (i.e. the median player has a swinging strike rate about five percentage points higher against 95+ mph fastballs). The wOBACON plot is similarly shaped, but around 0 (i.e. the point where there is no difference between a player’s wOBACON when you bucket balls in play by velocity faced). The shapes of both figures indicates one of two things: either the talent in facing higher end velocity (relative to lower velocity) is normally distributed in MLB or the differences in the two figures are random around the league-wide mean. The fact that the wOBACON figure is centered around zero lends credence to the latter; intuitively players should not be expected to produce better against higher velocity. Therefore, using a threshold of 100 PAs, looking at a season’s worth of PAs is not sufficient in judging a player’s ability to face high end velocity. For swinging strikes, there is a bit more to consider given the center of the distribution is located at a value that makes sense; we expect (and know) that higher velocity pitches (all else being equal) result in more whiffs. But still, within a season, any difference that deviates from that medium is mostly noise because year over year, there is little to no consistency in those deviations\n\n\n\n\n\nThis whole post is a long-winded plea to caution analysts looking to find signal in any velocity-based splits when it comes to batting performance (this can undoubtedly be said for any analysis leveraging splits). We know facing higher end velocity is going to result in lower offensive performance on the whole(evident in the depressed offensive environment of the postseason). But there is little to no evidence that players are particularly adept or poor at facing high end velocity relative to lower velocity at the major league level. There might be something to explore when looking at minor league players, where the distribution of talent is much wider. Without extensive minor league pitch-tracking publicly available, however, this is not possible to verify at this point in time."
  },
  {
    "objectID": "posts/microstats-repeatability/index.html",
    "href": "posts/microstats-repeatability/index.html",
    "title": "Hockey Microstats Repeatability",
    "section": "",
    "text": "The proliferation of “microstats” in the public space has been a boon in our understanding of hockey players. These stats include entries/exits (the concept and tracking of which were pioneered by Eric Tulsky), entry denials, and various types of passing plays (which to my knowledge were first studied in depth by Ryan Stimson). Corey Sznajder has been tracking this phylum of statistics and releasing them to the patrons of his All Three Zones project for public consumption and analysis. Corey has been an invaluable in increasing our understanding of the various skills players display while on the ice and his data is readily available for public-facing analysts to dig into.\nThese statistics can tell us why or how a player comes about his production. But they do not approximate player value on their own. Jack Fraser, in a newsletter post from several years ago, cautions people in using microstats to assess player ability. He stresses that these measures events, what actually is happening on the ice. We know that at the team level certain passing plays and transition into and out of the neutral zone are indicators of successfully driving goal differential and generation from the work of Tulsky and Stimson. At the player level this is not necessarily true. There are plenty of players who are not tasked with transporting the puck up the ice that provide significant value to their clubs. Look no further than Jason Robertson. A player not necessarily the most fleet of foot, Robertson defers these responsibilities to his more dynamic linemates (most notably Roope Hintz) and does his part by making himself available to receive passes in the offensive zone, where he can then put shots on net or parley the reception into another pass in the offensive zone to scramble the opposition. I think any fan or analyst following the NHL the past few years would agree that Robertson is one of the better wingers in the league, but he certainly does not stand out when looking at Corey’s tracking of transition plays. At the other end of the spectrum, successfully generating exits and entries is not a one way ticket for effectiveness. Look no further than Max Domi.\nAll of this is to say is that these tracked statistics are not the be all and end all in evaluating hockey players. There are skills that are clearly not captured that help players drive goal differentials for their team. I will say that if a player stands out in all or most of the tracking statistics, there is a decent chance that player is doing something right; but not definitively.\nThat is my preamble and at some point I would like to look deeper into the relationships between microstats and player impact. I have some projects in the works that will hopefully help bridge this gap, such as leveraging the play-by-play tracking data Corey provides to see its effects on expected goal generation, but that is for another time (hopefully). For now, one item that interests me is the consistency of these stats year over year for players that change teams. I alluded to the fact that these measures are dependent on a player’s role to some extent. A player like Connor McDavid would likely find himself carrying the puck a ton no matter the team he played for, but that is an extreme case. Most players would be heavily influenced by their team context and correspondingly would see shifts in their microstats output should they change teams. At least that is my hypothesis. Luckily this can be tested. I know some work has been done by Charlie O’Connor and Garik16 looking at the intra-season stability of entry and exit proclivity. I am not aware, however, of analysis that has been at players looking at inter-season consistency, let alone players that change teams. So that is what I am going to do here.\nBefore a start, I should note a few caveats. Corey does not track every minute of every shift that a player has at 5v5. He is one person, that would be impossible. This will be have to be a source of noise that we will accept when looking at the results. I am choosing to use a 100 minute cutoff for each seasonal pair (i.e. the player has at least 100 minutes tracked at 5v5 in each season). The other main caveat is that I am not taking into account the effects associated with aging. I would imagine, like any other statistics imaginable, that microstats are subject to aging curves. For those curious, I will include some aging curves at the end for the metrics I believe are most consistent across seasons. Finally, I am only using data from the 2017 through the 2020 season because Corey has each of the metrics summarized in a nicely organized spreadsheet. I wanted to get this analysis out there so I did not feel like spending the time it would require to go through the raw play-by-play logs creating the requisite summaries for the 2021 through 2023 seasons. If any of Corey’s patreons (or Corey himself) are aware of a spreadsheet with all of the statistics summarized for the other seasons, let me know and I can rerun my code including the more recent data.\nNow let us look at the results. Based on the criteria I set, there are 1632 eligible player-season pairs. Of those 1632 pairs, 1262 are of players who played on the same team year over year (and thus, 370 who changed clubs). 1062 players in the set are classified as forwards and the remaining 570 are defensemen. The following represents the year over year correlations between various statistics tracked by Corey, split up by whether the player in question changed teams year over year.\n\n\n\n\n\n\n\n\nFirst up are the forwards, with the chart on the left representing forwards who changed teams in between seasons and the chart on the right representing players who were on the same team season over season. Players who stayed put, unsurprisingly, have the higher year over year correlations across the board. These stats (and really all measures of player actions/performance) are some mixture of player skill and context/role. Naturally we want to tease out and isolate player skill, but these measures are undoubtedly influenced by a player’s teammates and the role which his coach asks him to take on.\nThe stats related to transition (exits/entries) are most stable year over year as are the more general measures for passing plays and shot attempts tracked. As the specificity of the offensive play type increases, the correlations decrease given we are dealing with a smaller sample relative to the total sample for shots and passes. The stats tracked covering actions in the defensive zone are basically noise for forwards; these actions are extremely infrequent.\n\n\n\n\n\n\n\n\nThese charts for defenders are oriented in the same way as their forward counterparts; players who changed teams on the left, the rest on the right. Similar to the forwards, the transition stats are stickier; more specifically, the transition stats related to exits. Defenders are much more involved in moving the puck out of their own zone relative to their responsibility for gaining entry into the offensive zone. This will naturally lead to a stronger year over year relationship given this is a responsibility that is more uniformly shared across team contexts whereas the role defenders play in their team’s offensive attack is more variable depending on team structure and coaching. Defenders show much smaller correlations for offensive play-types. As alluded to above, they are less involved offensively and thus we are looking at a smaller sample of plays across seasons. This is also most likely the result of defenders having less control over offensive play than forwards.\nFor the analysis on the whole, it is worth pointing out that we are looking at correlations (\\(R\\)), not coefficients of determination (\\(R^{2}\\)). The latter represents the proportion of one measure that can explain the other’s (in this case seasons N and N+1) variance. Thus, a correlation of approximately 0.707 (\\(\\frac{1}{\\sqrt{2}}\\)) would mean that the prior season’s values explain half the variance in the current season. Take a gander at the prior two charts. Very few measures for both player types reach that 50% threshold. Correlations of about 0.316 would indicate just a 10% \\(R^{2}\\). Arbitrary endpoints and whatnot, but no matter how we slice it there’s very little signal year over year signal, especially for players who change teams. We can conclude that much of these microstats are more descriptive than prescriptive and we should be paying most attention to entries/exits at a high level, shot and passing generation on the whole (i.e. without segmenting by type). That is not to tracking these play types and entry defense is not important in our understanding of the game; there is plenty of research that shows these are essential to goal generation and prevention. Moreover, for individual players, it seems that many of them do not represent repeatable skill across seasons and are highly dependent on team context and good old-fashioned variance.\nFinally, I will leave you with some aging curves for the more stable metrics. If there are any metrics not included here, feel free to contact me and I can produce those on your behalf."
  },
  {
    "objectID": "posts/microstats-repeatability/index.html#overview",
    "href": "posts/microstats-repeatability/index.html#overview",
    "title": "Hockey Microstats Repeatability",
    "section": "",
    "text": "The proliferation of “microstats” in the public space has been a boon in our understanding of hockey players. These stats include entries/exits (the concept and tracking of which were pioneered by Eric Tulsky), entry denials, and various types of passing plays (which to my knowledge were first studied in depth by Ryan Stimson). Corey Sznajder has been tracking this phylum of statistics and releasing them to the patrons of his All Three Zones project for public consumption and analysis. Corey has been an invaluable in increasing our understanding of the various skills players display while on the ice and his data is readily available for public-facing analysts to dig into.\nThese statistics can tell us why or how a player comes about his production. But they do not approximate player value on their own. Jack Fraser, in a newsletter post from several years ago, cautions people in using microstats to assess player ability. He stresses that these measures events, what actually is happening on the ice. We know that at the team level certain passing plays and transition into and out of the neutral zone are indicators of successfully driving goal differential and generation from the work of Tulsky and Stimson. At the player level this is not necessarily true. There are plenty of players who are not tasked with transporting the puck up the ice that provide significant value to their clubs. Look no further than Jason Robertson. A player not necessarily the most fleet of foot, Robertson defers these responsibilities to his more dynamic linemates (most notably Roope Hintz) and does his part by making himself available to receive passes in the offensive zone, where he can then put shots on net or parley the reception into another pass in the offensive zone to scramble the opposition. I think any fan or analyst following the NHL the past few years would agree that Robertson is one of the better wingers in the league, but he certainly does not stand out when looking at Corey’s tracking of transition plays. At the other end of the spectrum, successfully generating exits and entries is not a one way ticket for effectiveness. Look no further than Max Domi.\nAll of this is to say is that these tracked statistics are not the be all and end all in evaluating hockey players. There are skills that are clearly not captured that help players drive goal differentials for their team. I will say that if a player stands out in all or most of the tracking statistics, there is a decent chance that player is doing something right; but not definitively.\nThat is my preamble and at some point I would like to look deeper into the relationships between microstats and player impact. I have some projects in the works that will hopefully help bridge this gap, such as leveraging the play-by-play tracking data Corey provides to see its effects on expected goal generation, but that is for another time (hopefully). For now, one item that interests me is the consistency of these stats year over year for players that change teams. I alluded to the fact that these measures are dependent on a player’s role to some extent. A player like Connor McDavid would likely find himself carrying the puck a ton no matter the team he played for, but that is an extreme case. Most players would be heavily influenced by their team context and correspondingly would see shifts in their microstats output should they change teams. At least that is my hypothesis. Luckily this can be tested. I know some work has been done by Charlie O’Connor and Garik16 looking at the intra-season stability of entry and exit proclivity. I am not aware, however, of analysis that has been at players looking at inter-season consistency, let alone players that change teams. So that is what I am going to do here.\nBefore a start, I should note a few caveats. Corey does not track every minute of every shift that a player has at 5v5. He is one person, that would be impossible. This will be have to be a source of noise that we will accept when looking at the results. I am choosing to use a 100 minute cutoff for each seasonal pair (i.e. the player has at least 100 minutes tracked at 5v5 in each season). The other main caveat is that I am not taking into account the effects associated with aging. I would imagine, like any other statistics imaginable, that microstats are subject to aging curves. For those curious, I will include some aging curves at the end for the metrics I believe are most consistent across seasons. Finally, I am only using data from the 2017 through the 2020 season because Corey has each of the metrics summarized in a nicely organized spreadsheet. I wanted to get this analysis out there so I did not feel like spending the time it would require to go through the raw play-by-play logs creating the requisite summaries for the 2021 through 2023 seasons. If any of Corey’s patreons (or Corey himself) are aware of a spreadsheet with all of the statistics summarized for the other seasons, let me know and I can rerun my code including the more recent data.\nNow let us look at the results. Based on the criteria I set, there are 1632 eligible player-season pairs. Of those 1632 pairs, 1262 are of players who played on the same team year over year (and thus, 370 who changed clubs). 1062 players in the set are classified as forwards and the remaining 570 are defensemen. The following represents the year over year correlations between various statistics tracked by Corey, split up by whether the player in question changed teams year over year.\n\n\n\n\n\n\n\n\nFirst up are the forwards, with the chart on the left representing forwards who changed teams in between seasons and the chart on the right representing players who were on the same team season over season. Players who stayed put, unsurprisingly, have the higher year over year correlations across the board. These stats (and really all measures of player actions/performance) are some mixture of player skill and context/role. Naturally we want to tease out and isolate player skill, but these measures are undoubtedly influenced by a player’s teammates and the role which his coach asks him to take on.\nThe stats related to transition (exits/entries) are most stable year over year as are the more general measures for passing plays and shot attempts tracked. As the specificity of the offensive play type increases, the correlations decrease given we are dealing with a smaller sample relative to the total sample for shots and passes. The stats tracked covering actions in the defensive zone are basically noise for forwards; these actions are extremely infrequent.\n\n\n\n\n\n\n\n\nThese charts for defenders are oriented in the same way as their forward counterparts; players who changed teams on the left, the rest on the right. Similar to the forwards, the transition stats are stickier; more specifically, the transition stats related to exits. Defenders are much more involved in moving the puck out of their own zone relative to their responsibility for gaining entry into the offensive zone. This will naturally lead to a stronger year over year relationship given this is a responsibility that is more uniformly shared across team contexts whereas the role defenders play in their team’s offensive attack is more variable depending on team structure and coaching. Defenders show much smaller correlations for offensive play-types. As alluded to above, they are less involved offensively and thus we are looking at a smaller sample of plays across seasons. This is also most likely the result of defenders having less control over offensive play than forwards.\nFor the analysis on the whole, it is worth pointing out that we are looking at correlations (\\(R\\)), not coefficients of determination (\\(R^{2}\\)). The latter represents the proportion of one measure that can explain the other’s (in this case seasons N and N+1) variance. Thus, a correlation of approximately 0.707 (\\(\\frac{1}{\\sqrt{2}}\\)) would mean that the prior season’s values explain half the variance in the current season. Take a gander at the prior two charts. Very few measures for both player types reach that 50% threshold. Correlations of about 0.316 would indicate just a 10% \\(R^{2}\\). Arbitrary endpoints and whatnot, but no matter how we slice it there’s very little signal year over year signal, especially for players who change teams. We can conclude that much of these microstats are more descriptive than prescriptive and we should be paying most attention to entries/exits at a high level, shot and passing generation on the whole (i.e. without segmenting by type). That is not to tracking these play types and entry defense is not important in our understanding of the game; there is plenty of research that shows these are essential to goal generation and prevention. Moreover, for individual players, it seems that many of them do not represent repeatable skill across seasons and are highly dependent on team context and good old-fashioned variance.\nFinally, I will leave you with some aging curves for the more stable metrics. If there are any metrics not included here, feel free to contact me and I can produce those on your behalf."
  },
  {
    "objectID": "posts/predictive-woba/index.html",
    "href": "posts/predictive-woba/index.html",
    "title": "Developing a Predictive wOBA Measure",
    "section": "",
    "text": "Recently I read a blog post from Tom Tango at his personal site where he constructs a more predictive version of wOBA. I encourage you to give it a read, but I will quickly try to summarize his points and findings for those of you who may not have the time. He makes note of how xwOBA as found on Baseball Savant should be used: as an estimate for current wOBA based only on exit velocity and launch angle. He points out that this measure is often misinterpreted by public fans and analysts alike; it was/is not a prediction for future wOBA. Tango then goes on to construct a crude predictive wOBA (which is meant to be a player’s “true talent”) by bucketing batted balls by exit velocity and launch angle, taking the average wOBA of each bucket (which he refers to as the predictive wOBA for each bucket), and aggregates those values based on frequency for each player season. In the end, this measure proves to have a stronger correlation to the next season’s wOBA compared to both wOBA on its own and xwOBA. He concludes the post saying this method can be improved upon by incorporating the total batted balls in the sample, since one can surmise that the outcomes are more indicative of talent as the sample of batted balls increases.\nSo I attempted to do just that, with a slight twist by incorporating some more advanced modeling techniques. I started off with taking each player season since 2015 (the advent of publicly available batted ball information), bucketing the batted ball information as Tango did in his post, and adding some other measures. For those of you who did not read Tango’s original post, here is a key for how each batted ball is classified:\nBucket #\n      Exit Velocity Range (mph)\n      Launch Angle Range (°)\n    \n  \n  \n    1\n95 and Under\nOver 32\n    2\n95 and Under\nBetween 32 and 8\n    3\n95 and Under\nUnder 8\n    4\nBetween 100 and 95\nOver 32\n    5\nBetween 100 and 95\nBetween 32 and 8\n    6\nBetween 100 and 95\nUnder 8\n    7\nBetween 105 and 100\nOver 32\n    8\nBetween 105 and 100\nBetween 32 and 8\n    9\nBetween 105 and 100\nUnder 8\n    10\nOver 105\nOver 32\n    11\nOver 105\nBetween 32 and 8\n    12\nOver 105\nUnder 8\n  \n  \n    \n      Table theme via cbbdata\nAnd here is a sortable table with the data I am using for the model, which includes all players who recorded a batted ball. I will account for this information in the modeling process.\nWe have the data in hand, now we can model. The model will incorporate strikeout rates, walk rates, hit by pitch rates, the number of batted balls, the the frequency with which each player’s batted balls fall into the 12 aforementioned buckets. The target is going to be the following the player’s following season wOBA. I am going to use an XGBoost model for the regression and will tune the hyperparameters using a simulated annealing process from the handy finetune package. Let’s see how it looks:\nmultisession:\n- args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n- tweaked: FALSE\n- call: plan(multisession)\nLooks like diminishing returns as time went along, with a few outliers thrown in. Here are the five best models trained (based on RMSE):\nmtry\n      trees\n      tree_depth\n      learn_rate\n      rsme\n      n\n      std_err\n      iter\n    \n  \n  \n    1\n565\n8\n0.007817092\n0.08908047\n5\n0.002578514\n30\n    2\n742\n4\n0.006236565\n0.08912393\n5\n0.002867103\n16\n    4\n950\n4\n0.005835562\n0.08933424\n5\n0.002733826\n19\n    4\n742\n3\n0.010491683\n0.08937084\n5\n0.002771974\n17\n    5\n806\n5\n0.005438183\n0.08940414\n5\n0.002537927\n18\nWe have our list of models, so now we can select the best performing set of hyperparameters and get a final fit with feature importances.\nStrikeout rate is the most important feature in the model, followed by five features closely clustered together: walk rate and the frequency of buckets 3, 7, 8, and 11. Recall the key I provided above; bucket 3 corresponds to softly hit groundballs, bucket 7 solidly struck fly balls, bucket 8 solidly struck balls in the “sweet spot”, and bucket 11 the hardest hit batted balls in the “sweet spot” range. What can we draw from this? Plate skills (in the form of strikeout and walk rates) are extremely important in predicting next season’s wOBA for a player because, getting on base is good (shocker) and maximizing walks and minimizing strikeouts lead to getting on base. Second, those measures are incredibly stable year over year relative to other measures of player performance. As for the batted balls, it seems based on this analysis that avoiding poor contact on the ground, middling contact in the air that is likely to be caught be outfielders, and making hard contact at ideal launch angles is the best way to maximize production on contact. Nothing groundbreaking here, but it is nice to see that play out when analyzing the model.\nFinally we get to the models viability. The number to beat here is a \\(R^2\\) of 0.3704 with players who recorded 100 batted balls in both seasons (based on Tango’s blog post). How does this new model compare?\neval_tib &lt;- tib %&gt;% \n  bind_cols(predict(final_woba_mod, new_data = tib)) %&gt;% \n  filter(is.na(woba_next) == FALSE) %&gt;% # remove players who do not have a following season\n  rename(predicted_woba_next = .pred)\n\neval_tib &lt;- eval_tib %&gt;% mutate(bbe_n = as.double(bbe_n))\n\n# r-squared without filtering for guys with 100 BBEs\nprint(\n  paste(\"The coefficient of determination for the relationship between predicted and actual next season wOBA is\", \n  cor(eval_tib$woba_next, eval_tib$predicted_woba_next)^2 %&gt;% round(digits = 4), \n  sep = \" \"\n  )\n)\n\n[1] \"The coefficient of determination for the relationship between predicted and actual next season wOBA is 0.6091\"\n\n# filter for guys with 100 BBEs\nprint(\n  paste(\"The coefficient of determination for the relationship between predicted and actual next season wOBA for players with 100 BBEs in each season is\", \n  cor(eval_tib %&gt;% filter(bbe_n &gt;= 100, bbe_next &gt;= 100) %&gt;% .$woba_next, \n      eval_tib %&gt;% filter(bbe_n &gt;= 100, bbe_next &gt;= 100) %&gt;% .$predicted_woba_next)^2 %&gt;% \n    round(digits = 4), \n  sep = \" \"\n  )\n)\n\n[1] \"The coefficient of determination for the relationship between predicted and actual next season wOBA for players with 100 BBEs in each season is 0.6527\"\nNot bad (especially considering I did include any information about park factors or aging)! We cleared the bar that Tango set in his post before filtering for players with 100 batted balls, though obviously the degree of rigor with which I approached the problem was much different than Tom’s (who was concerned more presenting the concept of a predictive wOBA measure). He also did not incorporate walk or strikeout rates which feature prominently in this model. Nevertheless, I think this exercise here was a reasonable method of picking up where Tango left off by leveraging some more powerful modeling techniques. Just as important as being a good predictor of the future, analyzing the model yielded some important (though not necessarily novel) conclusions; mainly hitting the ball on the ground softly is bad, hitting fly balls that outfielders can easily catch is bad, and hitting the ball hard at optimal launch angles is the best way to ensure batted ball success.\nThere is one last note I want to make. I know there is some quibbling in the public space about not incorporating batted ball direction (in terms of spray angle) into a wOBA model. I would say two things can be true at the same time. The first is that directly incorporating the spray angle of a batted ball might make a model’s estimation of that singular batted ball more accurate, but I have found that when incorporating spray angle and aggregating the model outputs at the player level does not increase the predictiveness of the model, similar to the folks at MLBAM. That can be something I post about in the future (feel free to bother me on Twitter to get around to writing up the post). Whether this is the result of most players not being able to “cheat” their exit velocities consistently year over year by pulling the ball down the line, exit velocity and launch angle being so much more important than spray angle over time, or having a model with less parameters is going to introduce more sources of variance, I find it difficult to conclude spray angle must be included to make a batted ball model viable over a sufficient sample. At some point I plan at looking a pull rates, their repeatability, and how much control batters have over them (pitch location has some bearing on whether a ball is pulled as does how far in front of the plate the batter makes contact. Unfortunately the latter data is not available publicly).\nNow the second thing: spray angle does have some effect on exit velocity. Pulled balls result in higher exit velocities for a given player, so those who can consistently put the ball in the air will maximize whatever power they have in the tank (see Marcus Semien or Isaac Paredes). I know Robert Orr has done excellent work adjusting exit velocities for spray angle, as did Connor Kurcon before he was hired by the Astros. Alex Chamberlain also has written a great primer on the relationship between exit velocity and spray angle at FanGraphs. I think the way to bridge the gap between those concerned with ignoring spray angle and the fact that player-level predicted wOBAs seem to not be aided by the inclusion of spray angle is to incorporate exit velocities adjusted for spray angle and undergo a similar process as I have done above (add that to the list of things I want to work on). My hypothesis is that this will yield even more predictive results for player hitting skill, but I cannot know for sure until I do the work myself or someone decides to take it up themselves."
  },
  {
    "objectID": "posts/predictive-woba/index.html#appendix",
    "href": "posts/predictive-woba/index.html#appendix",
    "title": "Developing a Predictive wOBA Measure",
    "section": "Appendix",
    "text": "Appendix\nAt the end I wanted to include a table for those curious that includes wOBA, next season wOBA, and the predicted next season wOBA. I filtered for players with 50 batted ball events in each season and made it sortable and searchable."
  },
  {
    "objectID": "batter_table.html",
    "href": "batter_table.html",
    "title": "Batter Table",
    "section": "",
    "text": "This table contains batter rate stats for all pitches faced and is segmented by pitch type. To only look at all pitches faced, type “All” into the dialog box in the type column. All columns are sortable."
  },
  {
    "objectID": "posts/stephenson-cutter/index.html",
    "href": "posts/stephenson-cutter/index.html",
    "title": "Robert Stephenson’s Cutter",
    "section": "",
    "text": "A recent tweet from Lance Brozdowski where he wonders whether the change Robert Stephenson made going from Pittsburgh to Tampa was the result of altering the velocity of his slider to turn it into a cutter instead of changing the grip is a poignant one. If you cannot read the tweet or are resistant to clicking on the link, I will provide the two pictures below.\n\n\n\nPIT Grip\n\n\n\n\n\nTB Grip\n\n\nThis was a great pull and an interesting hypothesis. To take it once step further, I wanted to write up a quick post looking at the pitch characteristics and see if I can support Brozdowski’s assertion. Stepheson did not throw a cutter until he joined the Rays; with the Pirates he was basically a fastball-slider guy.\n\n\n\n\n\n\n  \n    \n    \n      Season\n      Team\n      CU\n      FF\n      SL\n      FS\n      FC\n    \n  \n  \n    2022\n\n2.4\n28.2\n69.4\n0.0\n0.0\n    2023\n\n0.8\n31.2\n62.9\n5.1\n0.0\n    2023\n\n0.0\n20.2\n6.7\n13.1\n60.1\n  \n  \n  \n    \n       % of total pitches thrown\n    \n  \n\n\n\n\nThe majority of his offerings with the Rays consisted of cutters, with a few more splitters thrown in. The slider was almost non-existent, especially relative to its usage in Pittsburgh. We know that the line between sliders and cutters is murky at a certain point in space, so was there really a substantive difference? I would definitively argue there was. The slider in Tampa came in at 85.3 MPH and in Pittsburgh 84.6, effectively identical. The cutter he threw at 88.7 mph on average. The movement patterns were also noticeably different, which Lance talked about in one of his YouTube videos.\n\n\n\n\n\nThe pitches colored yellow represent those from his time in Pittsburgh and correspondingly those in teal are from his time in Tampa. The cutters with the Rays are clustered in the top left, designating pitches with ride (i.e. backspin) and arm-side movement. There is some overlap with the pitches designated as sliders; the cutter can drift towards the glove-side with no magnus movement in the z-direction (almost a pure gyroball). The lack of consistency in the movement pattern might be construed as a negative but the pitch garnered a 59.9% whiff rate. For a pitch to be thrown that often and generate swings and misses at that rate, you can see why Stephenson struck out so many batters in Tampa.\nSo the new pitch is good. Great. But back to the original question and point of the post: What is he doing differently if the grips between the two pitches look the same? There has to be some difference, given the distinction in movement profiles. We know he’s throwing the pitch with more velocity. I explained that above. But there is something more going on here.\n\n\n\n\n\nAt least based on the 2D spin axes captured by the HawkEye cameras, the cutter and slider have functionally the same spin axis, but the ball is spinning in opposite directions.\n\n\n\n\n\nNow here is what the spin axes look like based on the movement of the pitches (credit to Dr. Alan Nathan for his work enabling me to perform these calculations). This mirroring phenomenon is still present in the inferred axes from the pitch movement; both pitches seem to leverage some modicum of movement from seam-effects to the tune of about 30 degrees of axis differential between the actual and inferred 2D spin axes. Not only is this the same for the cutter and slider, but the absolute deviations are also in the same direction (clockwise from this perspective). One last aspect of the pitches I wanted to check was the estimated amount of gyroscopic spin (i.e. bullet, or football-type spin) using “the angle of the spin with respect to the x − z plane” (again, based on Dr. Nathan’s work). 90 degrees represents pure gyroscopic spin, 0 represents pure transverse spin.\n\n\n\n\n\nThese pitches similarly have the vast majority of their spin in the form of gyroscopic spin, which is not surprising with their movement profiles clustering around the (0, 0) point in the short-form movement plot above.\nI said at the top this was going to be a short post, so I will leave you with this. Stephenson’s slider and cutter are very similar. They are thrown with similar amounts of gyro spin, the movement profiles are close to each other, they probably have similar movement bumps from seam-effects. The thing is, while they spin around the same axis, they do so in the complete opposite direction! For all the similarities, they are actually mirrors of each other. How did the Rays coax this out of him and help develop one of the the best pitches in baseball that yielded Stephenson an eight figure, multi-year guaranteed contract? Was it simply telling him to throw the ball with more velocity and this phenomenon was a natural byproduct? Or is there more at play here? Perhaps changing the distribution of pressure he’s putting on the ball with his index and middle fingers, where the cutter gets more backspin by having the index finger apply more pressure on the ball relative to the slider? That is a lot of questions to conclude a piece, but that is all we can really do. Without Stephenson or the Rays going on the record documenting what happened after Stephenson was traded to Tampa, it is impossible to know the tweak. Nevertheless, an interesting development and a great catch by Lance Brozdowski."
  },
  {
    "objectID": "skater_ev_rapm_tables.html",
    "href": "skater_ev_rapm_tables.html",
    "title": "Skater EV RAPM Tables",
    "section": "",
    "text": "Overview\nThe following three tables contain NHL even strength skater reguarlized adjusted plus-minus (RAPM) data in three, five, and one-year stints. The impacts are in terms of expected goals (xG) per 60 minutes of play using my own model. RAPM leverages a technique called ridge regression to estimate player impacts on offense and defense in the roles they are given. All the models have to go off of is what happened on the ice. They cannot extrapolate what would happen if a player had a different role, different deployment patterns, or played with different teammates. The quality of the teammates and opposition are accounted for of course, but if a player happens to line up with teammates who’s skills do not match up well with his own, tough luck. For more reading about RAPM, it was first popularized in NBA analysis and I would recommend reading this two part series from Justin Jacobs, who has consulted for NBA teams. If you prefer a more hockey-centric overview, this overview from the EvolvingWild twins is a great read.\nThese estimates are rate stats (per 60 minutes of even strength ice time). Two players who are close in estimated impact might look like they provided the same on-ice value on the surface, but if one player played significantly more minutes than another, that player provided more to his team. That does not make the player “better” per se, the impacts are meant to measure quality by themselves. But grizzled internet sports analysts know value is a function of both quality and opportunity.\nIf you read the overviews I linked, you will realize tha RAPM values are still very noisy over one year samples. It is best to use larger samples when using RAPM to evaluate players. Five years is the best bet, but that is also an unsatisfyingly long time horizon. Three year samples split the proverbial baby in this regard, having a much more robust sample of play-by-play data to work from than one-year samples but providing results quicker than the five-year RAPM.\nThe main reason for this type of model struggling to stabilize when fed a season’s worth of play-by-play data is that one season does not seem to be quite enough to adaquately dampen the effects of collinearity. Collinearity takes the form of pairs or groups of teammates who do not see a significant amount of ice time apart and the model correspondingly has trouble isolating those players’ impacts. Three (and certainly five) seasons give each player in the sample many chances to play with a variety of different teammates, due to line/pair changes throughout a multi-year period and roster turnover during the offseason and at the trade deadline.\nWhile we are on the topic of model limitations, there are a couple of items that I think are worth mentioning that I am sure one might question when looking at the results stored in the tables below. The first is that this model is trained to isolate impact in terms of expected goals. The usual caveats for expected goals apply here; obviously the publicly available data does not consider things like passing and goaltender movement. But let us not make perfect the enemy of good, we still have public access to important features such as shot distance and angles, the changes in each between shots, the time between shots, and shift start locations. I think directionally public expected goal models do a good job in bettering our understanding of what happens on the ice. As the British statistician George Box once said, “All models are wrong, some are useful.”\nAlong the same lines, we know players have some ability to drive goals above expected, whether via their own shot-making talent or their ability to complete high-danger passes (across the royal road, into the slot, to spring rush chances, etc). That talent is not totally captured in here. We would expect good passers to assist in generating higher xG shot attempts (which is not nothing) but players who can consistently outpace their individual expected goals with supreme shooting talent are not credited here (th most notable examples here would probably be a Leon Draisaitl or a Steven Stamkos). Shooting is definitely a skill but the extent to which it is repeatable over a large sample of shots is up for debate. Extreme cases,however, lead to some middling results for players we (the public) generally think of as elite talents. Those players are punished here and it should be documented.\nOne final note is that this does not include individual player contributions explicitly. There is no information about a player’s on-ice percentages here or their individual contributions (in terms of scoring, shot attempt, scoring chance, or microstat rates). The only information fed to the model is who is on the ice and contextual factors about the game and shift at hand (home/away, shift start location, is the game a back-to-back, score, etc.) Supplying these individual stats would give more insight into player roles and would most likely help the model distinguish who is driving the bus, as it were. It is reasonable to conclude that the model would be more successful in isolating and gauging player impact. I agree with this sentiment and admit it is the main limitation of the pure RAPM approach. In the future I would like to address this concern and see which players improve/worsen with the addition of more player-specific data. That is a project for another day.\nI will include tables for one-year, three-year, and five-year values and order them by my subjective view of their usefulness. The data included runs up through the games the last game of the 2023-24 regular season.\n\n\nThree-Year Table\n\n\n\n\n\n\n\n\n\nFive-Year Table\n\n\n\n\n\n\n\n\n\nOne-Year Table"
  },
  {
    "objectID": "posts/primary-matchups/index.html",
    "href": "posts/primary-matchups/index.html",
    "title": "NHL Primary Matchup Utilization",
    "section": "",
    "text": "A recent tweet thread from Micah Blake McCurdy, one of the foremost practitioners of public-facing hockey analysis, sparked an idea that I thought was worth researching. Micah was looking at rolling out a matchups visualization, where the primary matchups between opposing forward lines were displayed (at even strength) and included the total time of the matchup and the xG generated by each side. He noted that this is a difficult part of the game to visualize because the most common matchups in a given game do not take up a large percentage of the total ice-time. He followed that thought up with an inquiry of whether this exercise would be more useful come playoff time. I thought this would be worthwhile question to research. We hear a lot of talk from pundits and analysts that in the playoffs coaches are able to more heavily scout their opponents and correspondingly are much more particular in the matchups they want to see deployed. Does that sentiment hold up to scrutiny?\nI collected the play-by-play data from all regular season and postseason games in the 2021 through the 2023 season at 5v5 using the invaluable hockeyR package. I created shift stints by segmenting the data based on the five players players on the ice for each side. When one of the ten players left the ice, a new shift index would be created and a new shift would start. Following the shift segmentation, I grouped the data by shift and calculated the shift length in seconds.\n\n\n\nExample of data segmented by shift\n\n\nI pulled player information from Natural Stattrick to get player positions because I was interested in isolating the forwards who were on the ice for each shift. With the name and position information in tow, I could join to the shift table and pick out who were the forwards for each team at any given time. I then isolated the forwards on the ice for each team and pulled them out to create a new table. For every set of forward line pairs opposing each other, I calculated the total time they spent playing against each other in each game. I also calculated the total amount of relevant ice time in each game, so I could calculate the percentage of the time tracked that a matchup was on the ice. The data looked as follows at this point:\n\n\n\nExample of primary matchups data frame\n\n\nI defined the primary matchups as the top three forward line matchups in a game. This was an arbitrary selection, but I am skeptical that the conclusions drawn would fundamentally change if that number were two, four, or five. Before I report the results I want to make a couple of notes. The first is that the “relevant” data I reference met the following criteria: five skaters on the ice for each team, three skaters were forwards, and two were defensemen. There were points in time when there would be four forwards out on the ice, often when powerplays expired or there was a weird line change where something happened when four forwards happened to be on the ice. I did not include those instances in the analysis nor in my calculation of the total ice time. Furthermore, there were also instances where certain player slots (i.e. home player 1, 2, 3, etc) were left blank. I was not sure how to properly remedy these holes in the data, so I left the shifts where there was a missing player(s) out of both the collection of matchups and the calculations of the total time. By removing these two quirks in the data from both the primary matchup calculations and the total relevant time calculations, I figured these instances would wash out without comprising the integrity of the analysis. If anyone has any suggestions on how to deal with the missing data, I am all ears and could definitely rerun the code.\nNow for the most interesting part: are players deployed in a more pointed manner in the playoffs relative to the regular season? The answer, to my surprise, seems to be no.\n\n\n\n\n\n\n  \n    \n    \n      Season Type\n      Most Used\n      Second Most\n      Third Most\n    \n  \n  \n    Playoffs\n12.47%\n8.75%\n6.80%\n    Regular Season\n12.00%\n8.73%\n6.97%\n  \n  \n  \n\n\n\n\nThere is functionally no difference. The main matchup sees about half a percentage point more ice time, the second most common matchup is unchanged, and the third most used matchup actually sees almost a fifth of a percentage point more time in the regular season. I thought this could be some artifact of most shift stints being on the fly, where coaches have less control of against whom they deploy their players. One coach has the ability to deploy his players after he sees who the opposition ices before a faceoff. I thought this was something worth checking for. I filtered the shift stints data to only include shifts that started with a faceoff and re-ran the analysis and did not see any differences in results.\n\n\n\n\n\n\n  \n    \n    \n      Season Type\n      Most Used\n      Second Most\n      Third Most\n    \n  \n  \n    Playoffs\n21.58%\n13.81%\n10.22%\n    Regular Season\n20.66%\n13.67%\n10.46%\n  \n  \n  \n\n\n\n\nThe primary matchups take up a larger percentage of the shifts that start off with faceoffs in both the regular season and the playoffs but in terms of deviations there is virtually no change. Not the most exciting result. Player deployment based on matchups seems to not meaningfully change from the regular season to the postseason. Whatever combination of variables that coalesce into player ice time and the matchups we see might just be the same across game types. That is not to say that coaching staffs and front offices do not conduct more focused scouts of their playoff components compared to the regular season. That effort must manifest itself in a different way, most likely in tactical decisions in how to approach forechecks for/against, transition through the neutral zone, shot generation in the offensive zone, etc. But I think it is safe to say, until proven otherwise, that coaches do not try to match players up any differently in the playoffs than in the regular season and any analysis that suggests otherwise should be taken with a grain of salt."
  },
  {
    "objectID": "posts/secondary-assists/index.html",
    "href": "posts/secondary-assists/index.html",
    "title": "Commentary on Secondary Assists",
    "section": "",
    "text": "A recent question from the Hockey PDOCast Discord (link is in the show notes if you want jump in) about the usefulness of secondary assists across the overall population of NHL players sparked some debate. The merits of leveraging secondary assists in player analysis (at least relative to goals or primary assists; I am here for the argument that citing points as one of the more paramount measures in player analysis is not the most practical) is a well trodden topic of research at this point in time. For the uninitiated, secondary assist rates are less consistent year over year compared to both goal scoring and primary assist rates. If you do some digging, you can find analysis to this effect but to save readers the time I will link to one of these posts from Chace McCallum.\nNo need to elaborate any further. I would suggest to those who disagree with this premise to pursue some research on their own and report back with their findings. The real impetus for this post was a point one Discord user made about whether generalizing the usefulness of secondary assists across the entire player population is sensible. The user wondered whether secondary assists were a better measure of skill for the upper crust of the player pool because they are naturally more involved in their team’s offense and are more likely to complete high-end passing plays that do not directly result in subsequent shot and goal. I was skeptical of this claim. My rationale was the main sources of instability in secondary assists year over year still persisted no matter the quality of the player. Those sources are the lack of consistency in recording secondary assists and the degree to which the player is removed from the goal that is ultimately scored.\nLet me tackle the first item since it is a bit more straightforward. There are not any hard and fast rules for doling out assists as long as there is no change of possession between when the passer makes the pass and the goal scorers puts the puck in the back of the net. An assist is up to the scorekeeper’s discretion. That means that assists are subject to the variability and biases between and of scorekeepers. Primary assists are still subject to these issues, but I would argue there are more cases where it is exceedingly obvious to anyone watching a game that a primary assist occurred because the eventual goal comes within seconds of the pass occurring. This is not as clear with most secondary assists. Thus, if secondary assists are at the whims of scorekeeper biases more so than either goals or primary assists, you should expect more variability in player rates year over year (because the player has no control over these biases/variability).\nThe aforementioned second item (the degree to which the player is removed) is more conceptual. Throw out the idea of scorekeeper biases for a minute and assume that the definition of a secondary assist (and primary assist for that matter) is both set and clear. Now consider a goal that is scored that has both a primary assist and a secondary assist. For a player to pick up a secondary assist, five things had to occur:\n\nThe player who received the primary assist cleanly received the pass from the player who recorded the secondary assist\nThe primary assist player maintains possession and successfully gets a pass off to the goal scorer\nThe goal scorer cleanly receives the pass\nThe goal scorer shoots the puck on net\nThe puck reaches the back of the net\n\nI will note that goals and assists can be the result of deflections, but the general contours of the above scenario would still apply. For a player to receive a secondary assist on a goal, there is more out of his control relative to both the player receiving the primary assist and the player scoring the goal because he is further removed from the sequence defining action (the goal scored). The player with the secondary assist cannot control whether his teammate cleanly receives his pass, gets off a pass to another teammate, who then successfully puts the puck on goal and past the goaltender. There are more degrees of freedom in a system where secondary assists are considered, so more sources of variance. Therefore we should expect secondary assists to lag behind goals and primary assists in terms of year over year consistency because a lower proportion of the degrees of freedom in this goal-scoring system are directly the result of the actions of the player who receives the secondary assist.\nNow let’s get back to the premise of this post. These two issues outlined above apply to all players, no matter the skill level. Sure, a more skilled player is likely a better passer which leads to it being easier to receive that player’s passes. But by and large there is still a ton out of a high-skill player’s hands just as there is for his teammates playing at the bottom of the lineup. Nevertheless, the original question posed by the Discord member is both thought-provoking and demands a test. I sought to test the hypothesis by binning players into five groups (quintiles) based on several measures and check to see whether there is a similar degree of repeatability year over year in each bucket. I limited my analysis to forwards who played at least 700 even strength minutes in both seasons in a given seasonal pair since the 2015 season.\nI chose average time one ice per game, scoring rate, and goal scoring rate over the entire 10 year period as the statistics with which to bucket the players. All of the data was pulled from Natural Stattrick. The reasons for using point scoring and goal scoring rates is that they are the manifestation of offensive skill (to some degree, as I allude to in the first paragraph). Turning to time on ice is less obvious. My rationale was that coaches (theoretically) allocate ice time based on merit, in that they put the players out on the ice most often who they believe most positively contribute to their teams pursuit of outscoring the opposition. The degree to which ice time is correctly allocated based on this principle is obviously up to debate for some coaches more than others (see Ehlers, Nikolaj). But on the whole, I think it is safe to say that generally good players receive more ice time than less good players.\nA more enlightened reader might ask: “Hey Carmen, you even admitted that using scoring rates as a primary means of evaluating players is fraught with issues and we have better ways to measure player ability like publicly available WAR metrics or penalized regression-based RAPM rates. Why don’t you use those?” To that enlightened reader, I tip my cap to you and admit that binning based on my chosen metrics is likely not the best way to group players (if the goal is to best do so by ability). A more rigorous analysis would consist of me using my RAPM estimates or pulling WAR accumulation/rates from Evolving-Hockey. But I am lazy and Natural Stattrick has brilliantly laid out and easy to use tables for all of us to enjoy. So I saved myself time and pulled what I thought were the most applicable measures from there, with an understanding that there was some limitations to my methodology. I would assert that using what is ostensibly a better and more holistic stat would not likely change the conclusions drawn from my analysis. But I encourage those skeptical to give it a whirl themselves.\nOK, now some results. Across all three methods for separating players, there is no discernible difference between the reliability year over year of secondary assist rates for forwards.\n\n\n\n\n\n\n\n\n\n\n\nNo matter how you slice it, there is not much there. Player skill level does not seem to have a bearing in how prolifically a player is going to continue to accumulate secondary assists from one year to the next. The issues inherent in the attribution and accumulation of secondary assists I outlined in great detail are not immune to more productive players. They are inherent to how secondary assists are tracked by scorekeepers and earned by players. My initial skepticism of the claim was warranted, but that is not to say this was not a worthwhile endeavor. The original objection with the idea that secondary assists are not the same for players at either end of the lineup was one I had not heard of until this past weekend, nor one that I believe had been investigated. So, despite no interesting results, I think we all learned something new."
  },
  {
    "objectID": "posts/xg_spike/index.html",
    "href": "posts/xg_spike/index.html",
    "title": "Countering Opponent Flurries",
    "section": "",
    "text": "A recent question in an excellent breakdown of the Vegas/Dallas first round series on the Hockey PDOcast prior to game 6 sprung me into action and inspired this brief post. A little after the five minute mark of the episode Dimitri and his guest David Castillo (who does excellent work covering the Stars) wondered if Vegas is particularly adept at countering onslaughts from the opposition with its own offensive explosion. Another way of saying this is does the team in question have a propensity to succeed in both stopping an opponent’s “momentum” and developing “momentum” for themselves. Put aside the question of whether momentum in the context of a professional athletic contest actually exists (for that is a whole other can of worms) and let us consider the question at hand: Irrespective of whether it is something that is a “skill”, is this phenomenon something that has happened often with Vegas relative to other teams? Anecdotally, having watched a ton of Vegas the past couple of seasons (given its standing in the league and cup run in 2023), it does seem as if Vegas will have these scoring chance cavalcades that completely flip the script of a game. Do these tend to also occur after an opponent finds success generating offensive chances? That is something we could try to verify, provided that we have a set of assumptions to query the play-by-play data.\nThere is certainly not a “correct” method of doing this. But we have to settle on some method and a set of parameters. I looked at data from the past two seasons (I thought the spirit of the question was about this version of the Golden Knights, which I would consider the team they have iced during the cup run and into this year). I segmented each period of each game in four minute increments, giving me five segments for each team in each game. I defined an offensive spike as a four minute period when a team’s all-situations expected goals per 60 rate exceeded 5 (approximately 3.2 is average with my model in these two seasons). For every segment I had the offensive and defensive team in question along with the rates of expected goals, goals, and shot attempts accumulated. I joined a copy of the table to the end, where the join condition was that the next segment in the main table was the same as the segment in the table that was being joined and the defensive team in the first table was the offensive team in the second table. This allowed me to look at the offensive metrics of the attacking team in an entry alongside what the defensive team did on the offensive end in the following game segment. From there I filtered with the aforementioned xG condition and looked at how the defensive team performed in the next segment offensively with the following measures:\n\nThe frequency with which these conditions were met for each team\nThe percentage of following segments where the defensive team had an xG rate exceeding five of its own\nThe percentage of following segments where the defensive team had a goals scored rate exceeding five of its own\nThe average shot attempt rate in the segments in question\nThe average xG rates in the segments in question\nThe average goals scored rates in the segments in question\n\nThis yielded the following results:\n\n\n\n\n\n\n  \n    \n    \n      Team\n      n\n      xG Counter Spike %\n      Goal Counter Spike %\n      Shot Rate Following Spike\n      xG Rate Following Spike\n      Goal Rate Following Spike\n    \n  \n  \n    EDM\n274\n28.93\n23.86\n58.25\n3.88\n4.19\n    DAL\n224\n27.95\n25.47\n63.26\n3.93\n4.57\n    TOR\n233\n27.49\n23.98\n59.65\n3.72\n3.86\n    BOS\n239\n27.47\n26.37\n57.12\n3.34\n4.37\n    NJD\n224\n27.45\n21.57\n66.86\n3.92\n3.53\n    FLA\n281\n27.36\n25.37\n64.10\n3.86\n4.33\n    OTT\n293\n26.13\n16.67\n66.96\n3.72\n2.77\n    PIT\n270\n25.91\n17.62\n63.65\n3.65\n2.80\n    WPG\n237\n25.61\n23.78\n59.82\n3.70\n3.84\n    BUF\n285\n25.24\n22.82\n63.06\n3.43\n3.86\n    CGY\n218\n24.55\n18.56\n64.49\n3.26\n3.14\n    NYI\n269\n24.23\n18.56\n64.10\n3.47\n2.94\n    TBL\n257\n23.33\n23.89\n56.33\n3.00\n4.00\n    COL\n237\n23.08\n22.49\n61.78\n3.38\n3.82\n    NSH\n310\n22.27\n15.91\n61.50\n3.20\n2.52\n    CAR\n197\n21.90\n16.79\n66.35\n3.41\n2.63\n    MIN\n256\n21.74\n21.74\n60.49\n3.40\n3.59\n    DET\n228\n20.78\n15.58\n57.86\n3.03\n2.53\n    LAK\n216\n20.38\n28.03\n59.71\n3.18\n4.49\n    VAN\n262\n20.11\n21.20\n56.90\n3.11\n3.51\n    NYR\n241\n20.00\n19.41\n63.09\n3.33\n3.53\n    SJS\n285\n19.90\n13.27\n56.40\n2.87\n1.99\n    VGK\n247\n19.77\n19.21\n60.17\n3.25\n2.97\n    STL\n277\n19.17\n20.73\n56.35\n3.01\n3.34\n    SEA\n219\n18.87\n22.01\n58.58\n3.11\n3.40\n    CBJ\n300\n18.18\n15.45\n56.32\n2.68\n2.52\n    CHI\n292\n18.05\n15.61\n51.07\n2.83\n2.63\n    WSH\n254\n17.78\n20.00\n57.17\n2.86\n3.33\n    ARI\n312\n17.67\n17.67\n50.58\n2.98\n2.93\n    PHI\n260\n17.58\n20.88\n52.91\n2.75\n3.38\n    MTL\n329\n17.47\n19.65\n53.06\n2.67\n3.14\n    ANA\n376\n13.91\n19.55\n55.09\n2.61\n2.93\n  \n  \n  \n\n\n\n\nVegas does not stand out here. We see the best offensive teams overall generally populate the top of this leaderboard. Edmonton leads the way with countering an opponents xG spike with their own 28.93% of the time, followed by Dallas and Toronto. The Kings, oddly enough, top the list when you look at the same measure but instead of using xG you favor actual goals scored. But it’s worth noting that looking at goal scoring rates in small segments like this is far from best practice. Even when looking at the average rates and ignoring whether a counter spike occurred, Vegas does not fair well. The only way I could slice it where Vegas approaches the top of the league is by taking looking at average xG accumulated countering a spike with the condition that the segment in question is a spike (i.e. above 5 xG per 60). Maybe that’s a better way to look at this and isolating performance in those counter spike events specifically is more in the spirit of the question. I’m not sure, it seems like manipulating the data to confirm a prior (but I will add the table with the updated filtering condition at the bottom of this post, also note the tiny samples we are dealing with). But reasonable minds may differ! And like I said, there are many methods different from my segmentation method that would undoubtedly yield different results. One quibble a reader may have is by including all strength states, the sequences of the game with special teams on the ice will have an out-sized impact on these results in one way or another. I just worried that by removing about 20% of the data would make what is already not a massive sample even smaller. Again, reasonable minds, differing, etc. So if you have better methodology in mind, I urge you to pursue it yourself or let me know and I can look into it. Nevertheless, hopefully this answer to the original inquiry satiates PDOcast listeners and other hockey fans alike to some degree.\n\nTable with Rates Conditional on Counter Spike Occurring\n\n\n\n\n\n\n  \n    \n    \n      Team\n      n\n      Shot Rate Following Spike\n      xG Rate Following Spike\n      Goal Rate Following Spike\n    \n  \n  \n    MIN\n40\n91.12\n9.83\n7.88\n    NYR\n34\n98.82\n9.58\n10.59\n    DAL\n45\n95.67\n9.55\n10.00\n    FLA\n55\n97.36\n9.45\n9.27\n    VGK\n35\n96.86\n9.35\n6.86\n    CAR\n30\n96.50\n9.34\n7.00\n    NJD\n42\n95.36\n9.26\n6.07\n    WPG\n42\n94.29\n9.23\n6.43\n    NSH\n49\n97.35\n9.12\n7.35\n    TOR\n47\n87.45\n9.06\n7.34\n    COL\n39\n98.08\n9.06\n7.69\n    PIT\n50\n88.50\n9.00\n5.70\n    EDM\n57\n89.74\n8.97\n9.21\n    OTT\n58\n96.72\n8.92\n6.47\n    CHI\n37\n76.62\n8.88\n9.73\n    BUF\n52\n95.48\n8.84\n8.37\n    NYI\n47\n96.06\n8.81\n7.34\n    SEA\n30\n88.00\n8.77\n6.00\n    ARI\n38\n72.63\n8.75\n7.89\n    ANA\n37\n90.81\n8.65\n7.30\n    CGY\n41\n95.49\n8.59\n8.05\n    LAK\n32\n86.25\n8.51\n9.84\n    VAN\n37\n95.68\n8.34\n7.30\n    PHI\n32\n73.59\n8.22\n10.78\n    WSH\n32\n83.91\n8.17\n9.38\n    TBL\n42\n89.64\n8.08\n9.29\n    SJS\n39\n82.69\n7.90\n6.15\n    MTL\n40\n80.62\n7.77\n7.50\n    DET\n32\n88.59\n7.76\n6.09\n    CBJ\n40\n87.00\n7.70\n5.62\n    BOS\n50\n82.50\n7.67\n8.10\n    STL\n37\n84.73\n7.53\n8.11"
  },
  {
    "objectID": "posts/bad-playoff-performance/index.html",
    "href": "posts/bad-playoff-performance/index.html",
    "title": "Can We Identify Poor Postseason Performers?",
    "section": "",
    "text": "The Toronto Maple Leafs’ exit from the Stanley Cup Playoffs led to another round of fans and media alike casting aspersions on Toronto’s top players, notably Mitch Marner. The Leafs’ playoff disappointments in conjunction with Marner’s own perceived struggles have been a potent combination for one of the most individually scrutinized clubs in North American sports. This sort of criticism makes the assumption (and begs the question) that we can confidently tease out whether a player is a “playoff underperformer”. This also begs the question of whether there is something unique about the postseason crucible that makes players apt to underperform relative to our expectations of that player based solely on regular season performance. This quick post is going to explicitly attempt to answer the first question, but should address the second one given they are closely related. Doing any sort of analysis of samples as small as the Stanley Cup Playoffs (despite the games’ import) is full of pitfalls for reckless analysts and likely does not offer any interesting conclusions when approaching analysis with rigor.\nWithout delving into this at all, a reasonable prior when looking at players marked playoff failures is that they are the victim of misfortune. This is not to say that, based on less empirical methods (or “empirical” methods that do not actually hold up to scrutiny), one cannot come to the conclusion that a player performs poorly in the playoffs. Jack Han at his SubStack had a video outlining his case that there are inherent aspects of Marner’s game, posture, and style of play that lend themselves to playoff failure. I would recommend watching it and coming to your own conclusions, but I do wonder if this sort of analysis where you pull up poor on-ice and individual results in a small sample to criticize a player and then use video to confirm that assertion is problematic.\nNevertheless I wanted to see if there was anything I could do to find a point in time where we could be confident that a player was an underperformer. I took individual player data from the 2018 through the 2023 seasons and joined each player’s regular season and postseason scoring totals (in terms of points) and games played. Anyone familiar with my work or general outlook on the sport will know my opinions on using points to evaluate players (they are not positive). But, it’s easy data to pull and the reality is when players are being criticized for not showing up in the postseason, it is without a doubt almost always based on point totals. Given I am looking into these criticisms specifically, I thought it would make the most sense to meet people halfway.\nPlayers on average lost about 0.11 points from their per game scoring averages in the playoffs. This was weighted by the harmonic mean of playoff and regular season games played in a given season. For context that would make a point per game player in the regular season (so 82 points in 82 regular season games) a 73 point player over that same 82 game stretch. That is not a massive drop-off in my estimation, but it is not nothing. In this stretch of games I looked at regular season scoring has out-paced that in the playoffs, but that margin has lessened in recent vintage. My first method was looking at how a player’s per game scoring rate in the playoffs changes over time and then the gap between a player’s per game scoring in each setting changes as that player accumulates games. If you look at a simple scatter plot, there is not much to see.\n\n\n\n\n\nThis first image is just year over year changes in playoff scoring rates. There are close to an even amount of observations on either side of the x axis and the y values converge towards zero as more playoff games are accumulated. Just eye-balling it, the rate of change (i.e. the slope at any given point or the derivative) levels out at around 25 to 30 games. A similar phenomenon exists when looking at the career difference in scoring rates across game types.\n\n\n\n\n\nNot many conclusions to draw here; the results level out after a few seasons of playoff games (for most players given most teams do not make deep runs) and where you fall on this spectrum is most likely the result of random variance. I tried to look at correlations year over year for playoff underperformance, filtering out those who played less than 10 games in each season and found nothing. I even tried building a model where you look at career scoring differences to date and predict future differences, with case weights for games played. This resulted in something that looked reasonable, but when testing on out of sample data it was meaningless. Makes sense; dealing with samples this small should result in over-fitting if you are using a more robust modelling framework with a ton of variance. A more biased, simple model basically pops out results at the mean of the population that it was trained on. One other model I thought about but did not pursue was a survival regression trying to find at what point could we safely label a player a playoff underperformer. But I couldn’t think of any other features that would help prevent the model from over-fitting on just playoff scoring, which as we have seen is noisy over playoff-sized samples.\nMaybe with some more time thinking about the framework of the survival analysis I could develop something interesting, but for now I am going to leave it there. Despite my best efforts, I could not find any evidence of players having an intrinsic inability to perform in the playoffs, at least in terms of point scoring (which, as I said at the top, is generally the basis of this type of criticism). Playoff scoring averages and deviations from the regular season seem to converge around 30 games and the differences are minute for those players. This should be surprising to no one. Per game point scoring is dependent on so many factors outside of a player’s control (opposing goaltenders, teammate performance, and to some degree ice time) that when looking at samples that are often smaller than seven games (since those who “underperform” are generally not on teams that advance far in the postseason), expecting anything other than noise is foolish. The same logic applies to any other measure you look at whether it’s on-ice goal scoring/allowing rates or even the expected versions of those measures. Something like expected goals is still dependent on other players successfully making plays, completing and receiving passes, and firing shots on net. If there is anything to players having an inherent ability to not perform in the playoffs (whether it is the result of differences in play style compared to the regular season or the higher caliber of competition), doing the sort of analysis Jack did that I linked is your best bet. But that is very much subject to extreme amounts of confirmation bias. So, for your own sanity as a hockey fan, a player not showing out in the playoffs on the surface should not be cause for panic or consternation. That does not mean that player did not necessarily perform poorly; moreover, it is not evidence that the player is irredeemable nor a proverbial choke artist."
  },
  {
    "objectID": "posts/score-effect-remnants/index.html",
    "href": "posts/score-effect-remnants/index.html",
    "title": "Score-Effect Remnants",
    "section": "",
    "text": "A playoff game between the Rangers and Hurricanes on May 16th, 2024 sparked a question in regards to score effects. This was the deciding game in the series, a series in which the Rangers prevailed four games to two. At the start of the third period, the Rangers were down three goals to one. Martin Necas opened up the scoring with just 82 seconds left in the first period, followed by goals from Seth Jarvis, Vincent Trocheck, and Sebastian Aho all in first half of the second period. Anecdotally, play was pretty even between both sides despite the two goal deficit and the Rangers really opened it up offensively in the third period as they sought to chase down Carolina (as NHL teams are wont to do). What was curious to me was that even after tying the game, the Rangers appeared to push with the same offensive fervor that they were as they trailed by two goals.\nThis curiosity was the result of my basic understanding of what folks call score-effects. For the uninitiated, score-effects are the phenomena of, when controlling for the quality of the teams at hand, hockey clubs push more aggressively offensively when trailing and, correspondingly, cede territory and shots when defending a lead. Naturally this phenomena grows as the score differential grows, the consequence of the marginal value of a goal conceded lessening as the score differential increases. A team trailing by two goals is very likely to lose, as is a team trailing by three goals at any point in time. Thus, the trailing team does not care much for conceding a goal because it was already likely to lose down 2 goals while a goal scored would greatly increase its odds of winning. We often use score-effects to discount shots, chances, or expected goals generated by a team trailing. If teams behave in such a way where there are these measurable score-effects, one can surmise that it is much easier to generate offense when the other team is not much of a threat to return the favor.\nNow, let us get back to the game between New York and Carolina. The Rangers made a large comeback down two goals at the start of the third period but seemingly kept the pedal to metal (as it were) once the game was tied, eventually tacking on the game-winning goal with just under four minutes after the game-tying goal and sealing the contest and playoff series with an empty net goal in the final minute. And (again anecdotally) this did not seem to be a function of the Rangers receiving a visit from Lady Luck and opportunistically scoring on two of only a handful of shots; moreover they continued to dominate play even as and after the score was tied. This brings me to my question: is there such a thing as score-effect remnants? Meaning, when teams are chasing down a large deficit and happen to make a comeback, is there some sort of residual effect where the previously trailing team stays in a similar tactical lane? One might interpret this as a specific aspect or type of “momentum”, a (to put it gently) much debated topic in discourse across sports. Some of a certain persuasion might be rolling their eyes, while others might be baffled at the suggestion that momentum is not a driving force in deciding athletic contests. At the risk of angering folks, I thought it would be a worthwhile exercise to investigate the existence of these score-effect residuals and determine if my eyes were playing a trick on me as I took in the come-from-behind Rangers victory.\nI pulled play-by-play data from the 2021 through 2024 seasons. I searched for games where at some point one team in the contest was trailing by at least three goals. I then filtered for those games and in those games found the point where the score differential reached three and only considered the play-by-play data after that point. In these games I identified the trailing team in question and tracked its performance as the game progressed. As a proxy for performance, the metric of choice was 5v5 expected goals for percentage (xGF%) for the trailing team (for those unfamiliar, expected goals takes unblocked shot attempts and weights those shot attempts by the probability that the shot attempt results in a goal, so an xG of 0.1 indicates that the shot has a 10% chance of finding the back of the net). This process yielded the following results:\n\n\n\n\n\n\n  \n    \n    \n      Score Diff\n      n\n      Losing xG\n      Winning xG\n      xGF%\n    \n  \n  \n    1\n10899\n78.61\n79.28\n49.8\n    2\n30889\n199.82\n379.40\n34.5\n    3\n80144\n576.35\n534.72\n51.9\n    4\n34050\n237.61\n227.79\n51.1\n    5\n11757\n78.44\n75.16\n51.1\n    6\n3718\n25.85\n27.71\n48.3\n  \n  \n  \n\n\n\n\nThe column N indicates the number of entries in the play-by-play for each score differential. I filtered scores where the they did not appear on at least 3000 occasions for readability and sample size concerns. Clubs basically break even when the score differential is at least three and then there is a large dip in performance when they get a goal back. Then, following a second goal that brings the score within one, teams largely are trading chances at an even rate. What can we surmise from these results, especially the atrophy in performance when the deficit is cut by one? I would consider what types of teams dominate this sample of play-by-play events. As you may imagine, more likely than not teams that find themselves trailing by at least three goals are largely, uh, really bad for lack of a more eloquent term. The 2024 Sharks were much more likely to be losing by three goals at any point in a contest compared to the 2024 Panthers. But, good teams are not immune to large deficits over the course of an 82 game regular season. So of the instances where a team is losing by three goals, those teams that bring the deficit back to one are more likely than not good teams that happen to have an off or unlucky night. I will also note with the 34.5% xGF%, that figure is so low that it is probably evidence in and of itself that we are not dealing with a sufficiently large sample to come to any firm conclusions. Even the worst teams should not be expected to perform so poorly against the best teams in any game in any given season on average.\nI would argue this table provides some indication that my pondering of the existance of score-effect residuals was misguided and the result of being swept up in the results of a singular outcome. Teams do not seem to perform better after coming back from a large deficit. Compare the results above to the results regardless of whether one team suffers a large deficit:\n\n\n\n\n\n\n  \n    \n    \n      Score Diff\n      n\n      Losing xG\n      Winning xG\n      xGF%\n    \n  \n  \n    1\n376825\n2,688.24\n2,654.41\n50.3\n    2\n178546\n1,277.06\n1,237.69\n50.8\n    3\n80144\n576.35\n534.72\n51.9\n    4\n34050\n237.61\n227.79\n51.1\n    5\n11757\n78.44\n75.16\n51.1\n    6\n3718\n25.85\n27.71\n48.3\n  \n  \n  \n\n\n\n\nThere is functionally no difference with the larger deficits and when looking at those two and one goal deficits we actually see an improvement, a function of diluting the effects of the worst teams in the league with the performance of the league on the whole. I should also point out that since we have a much larger sample of events to pull from, we do see losing teams have a slight edge no matter the deficit, which is not surprising given the existence of these score-effects. If there were no score-effects, we would not expect the losing team to outperform its leading counterpart because more likely than not in any given game the winning team would be expected to be better in a neutral situation, all else being equal.\nSo, unfortunately, it looks like there are no exciting conclusions to draw from my initial inquiry. The score-effects are the score-effects and teams do not seem to be able to buck that phenomenon even with the aid of the excitement associated with a comeback attempt. I guess you can put another tally in the column for the momentum truthers. I welcome any comments or questions about the methodology used in this analysis. It is likely far from perfect and I would imagine there are more rigorous methods to approach this question with. I was just filtering and binning play-by-play data and chose the three goal deficit arbitrarily. Another avenue could be to create a model with a term that serves as a proxy for a comeback with some clever feature engineering and you could look at the significance of the feature(s) that highlight these situations. For now, I will leave that to another intrepid analyst."
  },
  {
    "objectID": "posts/pp_importance/index.html",
    "href": "posts/pp_importance/index.html",
    "title": "The Importance of Powerplay Offense",
    "section": "",
    "text": "Last week, Travis Yost over at TSN wrote about the increase in correlation between powerplay effectiveness and overall team success. The natural conclusion is that an effective powerplay is, more so than ever, a driving force behind team success in the NHL. Correlation is not causation; if two measures are correlated that does not mean there is a cause and effect relationship between those two measures. Still, effective powerplays result in goals scored and goals scored help drive goal margins. The name of the game, ultimately, is outscoring the other team. If a team does that more often than not, we should expect that team will be successful. Logically it would indicate that this relationship is in fact causal. That is not to say a good powerplay is the be all and end all. There are other facets of the game that contribute to a team’s success. But Yost was isolating the powerplay and that is what we are going to do here as well. To recreate Travis’ work, I pulled team-level data from Natural Stattrick going back to the 2011-2012 regular season and specifically looked at goals scored and conceded rates at even strength and for special teams and calculated the correlations (just like Travis) between points percentage and those four rates.\n\n\n\n\n\nHere we can see that we have come to the same conclusion. The strength of the correlation between team success and powerplay scoring has generally been on the rise the past decade. The rest of the rates in question have stayed relatively stagnant, though scoring at even strength has slightly increased in import as the league on the whole puts more pucks in the back of the net. There are a couple of other items from this figure that are worth noting. The first are the strong correlations between goals conceded at even strength and success (the correlations for the goals conceded are negative because successful teams allow fewer goals). Obviously keeping the puck out of your own net is incredibly important. But measures of defense are more volatile year-over-year and there is less of a spread in the figures we see teams post because a team playing defense has less control over a given sequence that the offensive team. In season this matters a ton for wins and losses, but between seasons defensive success is much more difficult to project. Hence the focus on offensive results. That does not mean teams have no control over their defensive performance; moreover, it is muted relative to offense. That can be a post for another day.\nApologies for the brief digression. The second item is the tightening of the gap between powerplay and even strength offense. We do not see the same phenomena with penalty kill defense and even strength defense (most likely an artifact of what I was talking about above). We are now at the point that where powerplay offense provides close to as much signal for whether a team is good as even strength offense despite the fact that more than 80% of a given game, on average, is played at even strength. And that figure is rising in recent vintage.\n\n\n\n\n\nIn 2015 there is a clear change in officiating resulting in more game time played at even strength. The relative magnitude is small; we are talking a change on the order of a few percentage points. But this difference would make a careful observer suspect that, all else being equal, powerplay scoring should become a less important tool in a good team’s toolbox. That is very much not the case.\nWhat are the possible causes for this change? I believe the main culprit is personnel usage. In the past decade, the usage of four forwards on the powerplay has more the doubled from about 40% of powerplay ice time to more than 80%, based on research from Daniel Nugent-Bowman over at the Athletic. Any individual who even passively has watched the NHL over the past 10 years could cite the fact that four forwards has been the most prevalent lineup of choice on the powerplay. Nevertheless the degree to which this has become in vogue is staggering and for good reason. According to research from Matt Cane (now of the Devils) back in 2015, four forward units attempt about 10 more shots per hour compared to their three forward counterparts and post a shooting percentage 13% higher. The latter point is not particularly surprising. For forwards who played at least 500 minutes in all situations last year, the median shooting percentage was about 10%. Their peers on defense only posted a median rate of a shade above 4.2%. Forwards are more apt to take shots closer to the net and defensemen (with the exception of a few coaches/clubs that allow their weak-side defensemen to aggressively pinch down the wall) generally are content to fire shots from the blue line.\nSo teams are putting more forwards out on the ice, which results in more goals on the powerplay and thus more goals overall. Putting goals on the board is always a good idea especially when the downside risk on the powerplay is small in terms of the chances of allowing a goal the other way. In that same piece from Cane, he found that four forward units allow marginally more shot attempts with a shooting percentage about three percentage points worse. Put it all together and four forward units should be expected to yield about 2 more goals (in terms of goal differential, so accounting for the difference in offense conceded) per hour. To take it one step further, teams are not only putting more forwards out on the ice, but they are concentrating that playing time in their best offensive players. I used points scored as a proxy for isolating the top three forwards on each team going back to the 2011-2012 season. The best offensive players are not strictly the top point scorers. There is more to driving offense than scoring points. But as a proxy for this analysis I thought this would be a decent way to pull the top offensive threats for each club without pulling out the old RAPM model. And, as an aside, player deployment is more driven by point scoring than a more rigorous model for player evaluation, for better or for worse (narrator: for worse).\nBack to the analysis at hand. Taking these aforementioned players, I charted by season the average percent of each teams total powerplay time those players were on the ice. As you can see, there has been a sharp increase to the tune about about 20% more playing time.\n\n\n\n\n\nOver this time frame the percentage of minutes played at even strength has been stagnant and we can see the sharp increase in powerplay ice time. If the best players are playing more on the powerplay and we know that hockey is a strong-link game based on research from Dom Luszczyszyn of the Athletic and Alex Novet at Hockey Graphs, then it should come to no surprise that powerplay offense has started to become a more integral part of how teams win games. This manifests itself in the spread in powerplay goal scoring rates being much larger than even strength rates. Granted, the smaller samples we have of powerplays on the ice will lend itself to more variance, but undoubtedly the fact that the best players are increasingly on the ice during powerplays is going to make it so that there is larger spread in success (given the assumption that hockey is a strong-link game). As you can see, that spread has widened as teams have begun to optimize their personnel usage, while at even strength the spread has been static over time.\n\n\n\n\n\nWhat is likely the main culprit for the increased importance on powerplay scoring? I would argue it is player deployment. That is not to say this is the sole reason. Yost in his TSN piece mentioned tactics as another potential reason for these results and I would certainly agree. Teams more than ever are forgoing dumping the puck in favor of generating more controlled zone entries, which we know result in more scoring chances. This is something that can be studied from a multitude of angles and personnel usage is likely more than just the tip of the iceberg, but not the sole driver of the phenomenon. Digging into the data tracked by Corey Sznajder for his All Three Zones project would likely unearth even more insights. That is a project for another day. For now though, I believe we can safely assume that some not insignificant portion of powerplay importance is the direct result of these drastic changes in player usage over the past decade, so I will leave it there. If you have any questions, comments, or quibbles, feel free to reach out via any of the avenues outlined in the About tab of the site."
  }
]